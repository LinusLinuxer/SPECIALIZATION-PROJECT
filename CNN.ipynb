{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd85a770",
   "metadata": {},
   "source": [
    "# CNN\n",
    "This notebook aims to deploy the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "325a8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec81f6",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading the data\n",
    "Data has to load out of the 'carolianminuscule-groundtruth'-folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "70206626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(folder_path: str):\n",
    "    \"\"\"\n",
    "    Load images and text files from the given path.\n",
    "    :param folder_path: Path to the directory containing images and text files.\n",
    "    :return: Two lists - one for image paths and one for text file paths.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Path {folder_path} does not exist.\")\n",
    "\n",
    "    images = []\n",
    "    files = []\n",
    "\n",
    "    for entry in os.listdir(folder_path):\n",
    "        entry_path = os.path.join(folder_path, entry)\n",
    "        if os.path.isdir(entry_path):\n",
    "            # Recursively get images and text files from subdirectories\n",
    "            sub_images, sub_files = get_images(entry_path)\n",
    "            images.extend(sub_images)\n",
    "            files.extend(sub_files)\n",
    "        elif entry.endswith(\".png\"):\n",
    "            images.append(entry_path)\n",
    "        elif entry.endswith(\".txt\"):\n",
    "            files.append(entry_path)\n",
    "\n",
    "    return images, files\n",
    "\n",
    "\n",
    "# load the data from the directory\n",
    "path = \"carolineminuscule-groundtruth\"\n",
    "images, files = get_images(path)\n",
    "\n",
    "\n",
    "# matched the .png- and .txt-file in a folder together\n",
    "matched_list_path = [\n",
    "    [img, file]\n",
    "    for img in images\n",
    "    for file in files\n",
    "    if os.path.dirname(img) == os.path.dirname(file)\n",
    "    and os.path.splitext(os.path.splitext(os.path.basename(img))[0])[0]\n",
    "    == os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f715ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len matched: 429\n",
      "matched_list:\n",
      " ['carolineminuscule-groundtruth/bsb00095929/0011/010002.bin.png', 'carolineminuscule-groundtruth/bsb00095929/0011/010002.gt.txt']\n"
     ]
    }
   ],
   "source": [
    "print(f\"len matched: {len(matched_list_path)}\")\n",
    "print(f\"matched_list:\\n {matched_list_path[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e60f70",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8877200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataframe to store the image, image paths and their corresponding text files\n",
    "df = pd.DataFrame(columns=[\"name\", \"image\", \"transcription\"])\n",
    "\n",
    "for i, (img_path, file_path) in enumerate(matched_list_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    # convert the image to a numpy array\n",
    "    img = np.array(img)\n",
    "    # add the image to the dataframe, set \"none\" here to add transcription later\n",
    "    df.loc[i] = [os.path.basename(img_path), img, None]\n",
    "    # read the text file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # read the transcription\n",
    "        transcription = f.read()\n",
    "    # add the transcription to the dataframe\n",
    "    df.loc[i, \"transcription\"] = transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f1c8f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010005.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>initio sicuti pleriq; studio ad empabacan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010002.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>gla memores que s quis: faciliafacto putat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010007.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>pro abstinentia ꝓuirtute audacia. largitio. au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010008.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>bant. Quę tametsianimus aspꝑnabatur. insolens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010017.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>tilinę coniuratione quam uerissime potero paucis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                              image  \\\n",
       "0  010005.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "1  010002.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "2  010007.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "3  010008.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "4  010017.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "\n",
       "                                       transcription  \n",
       "0          initio sicuti pleriq; studio ad empabacan  \n",
       "1         gla memores que s quis: faciliafacto putat  \n",
       "2  pro abstinentia ꝓuirtute audacia. largitio. au...  \n",
       "3  bant. Quę tametsianimus aspꝑnabatur. insolens ...  \n",
       "4   tilinę coniuratione quam uerissime potero paucis  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"transcription\"] = df[\"transcription\"].str.replace(\"\\n\", \"\", regex=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d708f",
   "metadata": {},
   "source": [
    "For now it appear that the images only have \"255\" as values, i.e. white. That's why im checking for other values. But the edges of the images are all white, therefore this is the exspected behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ecfc47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with values other than 255: 429\n"
     ]
    }
   ],
   "source": [
    "non_255_values = df['image'].apply(lambda img: np.any(img != 255))\n",
    "print(f\"Rows with values other than 255: {non_255_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb1435",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cb64f",
   "metadata": {},
   "source": [
    "By calculating the maximum and avg. number of the height we can use that value later in our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4dcdfc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum height of an image: 263\n",
      "Average height of an image: 140.2004662004662\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum and average length of the lists in the 'image' column\n",
    "image_lengths = df['image'].apply(lambda img: img.shape[0])  # Get the height of each image array\n",
    "max_length = image_lengths.max()\n",
    "avg_length = image_lengths.mean()\n",
    "\n",
    "print(f\"Maximum height of an image: {max_length}\")\n",
    "print(f\"Average height of an image: {avg_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832590ff",
   "metadata": {},
   "source": [
    "Getting the number of unique character, server here more as an eploration into the the data. We stand at a pathway here: Either we manually map each of the 83 characters by hand to the according representative in the images or, use the CNN-RNN architecture. Which does not need manual character mapping, but is harder to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 83 unique characters.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m label_sequences = [[char_to_index[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m transcription] \u001b[38;5;28;01mfor\u001b[39;00m transcription \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33mtranscription\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Step 3: Pad sequences to match the model's output length\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m max_sequence_length = \u001b[43moutput\u001b[49m.shape[\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Use the second dimension of the model's output\u001b[39;00m\n\u001b[32m     22\u001b[39m padded_labels = pad_sequences(label_sequences, maxlen=max_sequence_length, padding=\u001b[33m'\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m'\u001b[39m, value=num_classes - \u001b[32m1\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Step 4: One-hot encode the padded labels\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a set of unique characters from the transcriptions\n",
    "# This is needed to set the number of classes in the model, if \n",
    "# you want to use a character-based model\n",
    "\n",
    "charset = set()\n",
    "for transcription in df['transcription']:\n",
    "    charset.update(transcription.strip())\n",
    "charset = sorted(charset)\n",
    "print(f\"The dataset contains {len(charset)} unique characters.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319becdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (images) and labels (transcriptions)\n",
    "X = df[\"image\"].values  # Images as NumPy arrays\n",
    "y = df[\"transcription\"].values  # Transcriptions as strings\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "num_classes = len(charset) + 1  # Number of unique characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551685d5",
   "metadata": {},
   "source": [
    "---\n",
    "The 'cnn_rnn'-model is comprised of an\n",
    "- input layer\n",
    "  - where images are read into with 64px, None and one channel\n",
    "- A convolutional layer which is building 64, 3x3-filters\n",
    "- Maxpooling to reduce parametersize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe5092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_rnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cnn_rnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,720,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,092</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m4,720,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)       │        \u001b[38;5;34m43,092\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,413,140</span> (24.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,413,140\u001b[0m (24.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,413,140</span> (24.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,413,140\u001b[0m (24.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, None, 1)),  # height, width, channel — width is variable\n",
    "        layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        layers.Reshape(target_shape=(-1, 128 * 16)),  # Adjust 16 if height is 64 and pooled twice\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True)),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ],\n",
    "    name=\"cnn_rnn\"\n",
    ")\n",
    "\n",
    "\n",
    "print(cnn_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660556de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reshaped shape: (343, 32, 32, 1)\n",
      "X_test_reshaped shape: (86, 32, 32, 1)\n",
      "y_train shape: (343,)\n",
      "y_test shape: (86,)\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Reshape the images to match the input shape of the model\n",
    "X_train_reshaped = np.array(\n",
    "    [cv2.resize(img, (32, 32)).reshape(32, 32, 1) for img in X_train]\n",
    ")\n",
    "X_test_reshaped = np.array(\n",
    "    [cv2.resize(img, (32, 32)).reshape(32, 32, 1) for img in X_test]\n",
    ")\n",
    "\n",
    "\n",
    "#DEBUG\n",
    "print(f\"X_train_reshaped shape: {X_train_reshaped.shape}\")\n",
    "print(f\"X_test_reshaped shape: {X_test_reshaped.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ad05303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, None, 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcnn_rnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/1_Universitaet/10_SoSe_25/SPECIALIZATION-PROEJECT/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/1_Universitaet/10_SoSe_25/SPECIALIZATION-PROEJECT/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:653\u001b[39m, in \u001b[36mcategorical_crossentropy\u001b[39m\u001b[34m(target, output, from_logits, axis)\u001b[39m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReceived: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m     )\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target.shape) != \u001b[38;5;28mlen\u001b[39m(output.shape):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    654\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArguments `target` and `output` must have the same rank \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    655\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(ndim). Received: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    656\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    657\u001b[39m     )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target.shape, output.shape):\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 != e2:\n",
      "\u001b[31mValueError\u001b[39m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, None, 84)"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "cnn_rnn.fit(\n",
    "    X_train_reshaped,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test_reshaped, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cfab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
