{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd85a770",
   "metadata": {},
   "source": [
    "# CNN\n",
    "This notebook aims to deploy the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb54145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325a8a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:19:54.706742: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-14 13:19:54.860110: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-14 13:19:54.950934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747221595.213402   12181 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747221595.237652   12181 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747221595.409213   12181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747221595.409238   12181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747221595.409240   12181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747221595.409241   12181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-14 13:19:55.428598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec81f6",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading the data\n",
    "Data has to load out of the 'carolianminuscule-groundtruth'-folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70206626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(folder_path: str):\n",
    "    \"\"\"\n",
    "    Load images and text files from the given path.\n",
    "    :param folder_path: Path to the directory containing images and text files.\n",
    "    :return: Two lists - one for image paths and one for text file paths.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Path {folder_path} does not exist.\")\n",
    "\n",
    "    images = []\n",
    "    files = []\n",
    "\n",
    "    for entry in os.listdir(folder_path):\n",
    "        entry_path = os.path.join(folder_path, entry)\n",
    "        if os.path.isdir(entry_path):\n",
    "            # Recursively get images and text files from subdirectories\n",
    "            sub_images, sub_files = get_images(entry_path)\n",
    "            images.extend(sub_images)\n",
    "            files.extend(sub_files)\n",
    "        elif entry.endswith(\".png\"):\n",
    "            images.append(entry_path)\n",
    "        elif entry.endswith(\".txt\"):\n",
    "            files.append(entry_path)\n",
    "\n",
    "    return images, files\n",
    "\n",
    "\n",
    "# load the data from the directory\n",
    "path = \"carolineminuscule-groundtruth\"\n",
    "images, files = get_images(path)\n",
    "\n",
    "\n",
    "# matched the .png- and .txt-file in a folder together\n",
    "matched_list_path = [\n",
    "    [img, file]\n",
    "    for img in images\n",
    "    for file in files\n",
    "    if os.path.dirname(img) == os.path.dirname(file)\n",
    "    and os.path.splitext(os.path.splitext(os.path.basename(img))[0])[0]\n",
    "    == os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f715ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len matched: 429\n",
      "matched_list:\n",
      " ['carolineminuscule-groundtruth/bsb00095929/0011/010002.bin.png', 'carolineminuscule-groundtruth/bsb00095929/0011/010002.gt.txt']\n"
     ]
    }
   ],
   "source": [
    "print(f\"len matched: {len(matched_list_path)}\")\n",
    "print(f\"matched_list:\\n {matched_list_path[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e60f70",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8877200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataframe to store the image, image paths and their corresponding text files\n",
    "df = pd.DataFrame(columns=[\"name\", \"image\", \"transcription\"])\n",
    "\n",
    "for i, (img_path, file_path) in enumerate(matched_list_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    # convert the image to a numpy array\n",
    "    img = np.array(img)\n",
    "    # add the image to the dataframe, set \"none\" here to add transcription later\n",
    "    df.loc[i] = [os.path.basename(img_path), img, None]\n",
    "    # read the text file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # read the transcription\n",
    "        transcription = f.read()\n",
    "    # add the transcription to the dataframe\n",
    "    df.loc[i, \"transcription\"] = transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1c8f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010005.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>initio sicuti pleriq; studio ad empabacan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010002.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>gla memores que s quis: faciliafacto putat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010007.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>pro abstinentia ꝓuirtute audacia. largitio. au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010008.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>bant. Quę tametsianimus aspꝑnabatur. insolens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010017.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>tilinę coniuratione quam uerissime potero paucis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                              image  \\\n",
       "0  010005.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "1  010002.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "2  010007.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "3  010008.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "4  010017.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "\n",
       "                                       transcription  \n",
       "0          initio sicuti pleriq; studio ad empabacan  \n",
       "1         gla memores que s quis: faciliafacto putat  \n",
       "2  pro abstinentia ꝓuirtute audacia. largitio. au...  \n",
       "3  bant. Quę tametsianimus aspꝑnabatur. insolens ...  \n",
       "4   tilinę coniuratione quam uerissime potero paucis  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete all the linebrakes in the transcription\n",
    "df[\"transcription\"] = df[\"transcription\"].str.replace(\"\\n\", \"\", regex=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d708f",
   "metadata": {},
   "source": [
    "For now it appear that the images only have \"255\" as values, i.e. white. That's why im checking for other values. But the edges of the images are all white, therefore this is the exspected behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecfc47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with values other than 255: 429\n"
     ]
    }
   ],
   "source": [
    "non_255_values = df['image'].apply(lambda img: np.any(img != 255))\n",
    "print(f\"Rows with values other than 255: {non_255_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb1435",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cb64f",
   "metadata": {},
   "source": [
    "By calculating the maximum and avg. number of the height we can use that value later in our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dcdfc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum height of an image: 263\n",
      "Average height of an image: 140.2004662004662\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum and average length of the lists in the 'image' column\n",
    "image_lengths = df['image'].apply(lambda img: img.shape[0])  # Get the height of each image array\n",
    "max_length = image_lengths.max()\n",
    "avg_length = image_lengths.mean()\n",
    "\n",
    "print(f\"Maximum height of an image: {max_length}\")\n",
    "print(f\"Average height of an image: {avg_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832590ff",
   "metadata": {},
   "source": [
    "Getting the number of unique character, server here more as an eploration into the the data. We stand at a pathway here: Either we manually map each of the 83 characters by hand to the according representative in the images or, use the CNN-RNN architecture. Which does not need manual character mapping, but is harder to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a40fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a character-to-index mapping\n",
    "charset = sorted(set(\"\".join(df['transcription'])))  # Unique characters in the dataset\n",
    "char_to_index = {char: idx for idx, char in enumerate(charset)}\n",
    "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
    "num_classes = len(charset) + 1  # Add 1 for the blank character (CTC loss)\n",
    "\n",
    "# Step 2: Convert transcriptions to sequences of integers\n",
    "label_sequences = [[char_to_index[char] for char in transcription] for transcription in df['transcription']]\n",
    "\n",
    "# Step 3: Pad sequences to a fixed length\n",
    "max_sequence_length = 32  # Adjust based on your dataset\n",
    "padded_labels = pad_sequences(label_sequences, maxlen=max_sequence_length, padding='post', value=num_classes - 1)\n",
    "\n",
    "# Step 4: Normalize and reshape the images\n",
    "X = np.array([cv2.resize(img, (128, 64)) for img in df['image']])  # Resize to (64, variable width)\n",
    "X = X / 255.0  # Normalize pixel values\n",
    "X = X.reshape(X.shape[0], 64, 128, 1)  # Add channel dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3c9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (images) and labels (transcriptions)\n",
    "X = df[\"image\"].values  # Images as NumPy arrays\n",
    "y = df[\"transcription\"].values  # Transcriptions as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556ff1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (429, 64, 128, 1), dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Convert images to NumPy arrays and normalize\n",
    "X = np.array(\n",
    "    [cv2.resize(img, (128, 64)) for img in df[\"image\"]], dtype=np.float32\n",
    ")  # Resize and convert to float32\n",
    "X = X / 255.0  # Normalize pixel values to [0, 1]\n",
    "X = X.reshape(X.shape[0], 64, 128, 1)  # Add channel dimension (grayscale images)\n",
    "\n",
    "print(f\"X shape: {X.shape}, dtype: {X.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acbf4ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (429, 32, 84), dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Convert transcriptions to sequences of integers\n",
    "label_sequences = [[char_to_index[char] for char in transcription] for transcription in df['transcription']]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 32  # Adjust based on your dataset\n",
    "padded_labels = pad_sequences(label_sequences, maxlen=max_sequence_length, padding='post', value=num_classes - 1)\n",
    "\n",
    "# Convert padded labels to one-hot encoding\n",
    "y = np.array([to_categorical(seq, num_classes=num_classes) for seq in padded_labels], dtype=np.float32)\n",
    "\n",
    "print(f\"y shape: {y.shape}, dtype: {y.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "319becdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "num_classes = len(charset) + 1  # Number of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56fbdc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (343, 64, 128, 1), dtype: float32\n",
      "y_train shape: (343, 32, 84), dtype: float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}, dtype: {X_train.dtype}\")\n",
    "print(f\"y_train shape: {y_train.shape}, dtype: {y_train.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551685d5",
   "metadata": {},
   "source": [
    "---\n",
    "The 'cnn_rnn'-model is comprised of an\n",
    "- input layer\n",
    "  - where images are read into with 64px, None and one channel\n",
    "- A convolutional layer which is building 64, 3x3-filters\n",
    "- Maxpooling to reduce parametersize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5fe5092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linus/Dokumente/1_Universitaet/10_SoSe_25/SPECIALIZATION-PROEJECT/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-05-14 13:20:02.330637: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,720,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,092</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m4,720,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)       │        \u001b[38;5;34m43,092\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,413,140</span> (24.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,413,140\u001b[0m (24.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,413,140</span> (24.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,413,140\u001b[0m (24.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_rnn = models.Sequential([\n",
    "    # CNN layers\n",
    "    layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(64, None, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Reshape for RNN\n",
    "    layers.Reshape(target_shape=(-1, 128 * 16)),  # Adjust based on pooling and input size\n",
    "\n",
    "    # RNN layers\n",
    "    layers.Bidirectional(layers.LSTM(256, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(256, return_sequences=True)),\n",
    "\n",
    "    # Output layer\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "cnn_rnn.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "cnn_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad05303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Commented out to avoid running the model training\n",
    "# # Fit the model\n",
    "# cnn_rnn.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=10,\n",
    "#     validation_data=(X_test, y_test),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dc01f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "cnn_rnn.save(\"CNN_RNN_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
