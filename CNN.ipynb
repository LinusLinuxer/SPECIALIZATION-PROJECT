{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd85a770",
   "metadata": {},
   "source": [
    "# CNN\n",
    "This notebook aims to deploy the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325a8a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 17:21:23.935979: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-13 17:21:24.061168: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-13 17:21:24.154237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747149684.244178  868052 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747149684.269003  868052 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747149684.450207  868052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747149684.450241  868052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747149684.450244  868052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747149684.450245  868052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-13 17:21:24.471802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec81f6",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading the data\n",
    "Data has to load out of the 'carolianminuscule-groundtruth'-folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70206626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(folder_path: str):\n",
    "    \"\"\"\n",
    "    Load images and text files from the given path.\n",
    "    :param folder_path: Path to the directory containing images and text files.\n",
    "    :return: Two lists - one for image paths and one for text file paths.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Path {folder_path} does not exist.\")\n",
    "\n",
    "    images = []\n",
    "    files = []\n",
    "\n",
    "    for entry in os.listdir(folder_path):\n",
    "        entry_path = os.path.join(folder_path, entry)\n",
    "        if os.path.isdir(entry_path):\n",
    "            # Recursively get images and text files from subdirectories\n",
    "            sub_images, sub_files = get_images(entry_path)\n",
    "            images.extend(sub_images)\n",
    "            files.extend(sub_files)\n",
    "        elif entry.endswith(\".png\"):\n",
    "            images.append(entry_path)\n",
    "        elif entry.endswith(\".txt\"):\n",
    "            files.append(entry_path)\n",
    "\n",
    "    return images, files\n",
    "\n",
    "\n",
    "# load the data from the directory\n",
    "path = \"carolineminuscule-groundtruth\"\n",
    "images, files = get_images(path)\n",
    "\n",
    "\n",
    "# matched the .png- and .txt-file in a folder together\n",
    "matched_list_path = [\n",
    "    [img, file]\n",
    "    for img in images\n",
    "    for file in files\n",
    "    if os.path.dirname(img) == os.path.dirname(file)\n",
    "    and os.path.splitext(os.path.splitext(os.path.basename(img))[0])[0]\n",
    "    == os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f715ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len matched: 429\n",
      "matched_list:\n",
      " ['carolineminuscule-groundtruth/bsb00095929/0011/010002.bin.png', 'carolineminuscule-groundtruth/bsb00095929/0011/010002.gt.txt']\n"
     ]
    }
   ],
   "source": [
    "print(f\"len matched: {len(matched_list_path)}\")\n",
    "print(f\"matched_list:\\n {matched_list_path[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8877200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataframe to store the image, image paths and their corresponding text files\n",
    "df = pd.DataFrame(columns=[\"name\", \"image\", \"transcription\"])\n",
    "\n",
    "for i, (img_path, file_path) in enumerate(matched_list_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    # convert the image to a numpy array\n",
    "    img = np.array(img)\n",
    "    # add the image to the dataframe, set \"none\" here to add transcription later\n",
    "    df.loc[i] = [os.path.basename(img_path), img, None]\n",
    "    # read the text file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # read the transcription\n",
    "        transcription = f.read()\n",
    "    # add the transcription to the dataframe\n",
    "    df.loc[i, \"transcription\"] = transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1c8f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010005.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>initio sicuti pleriq; studio ad empabacan\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010002.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>gla memores que s quis: faciliafacto putat\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010007.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>pro abstinentia ꝓuirtute audacia. largitio. au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010008.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>bant. Quę tametsianimus aspꝑnabatur. insolens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010017.bin.png</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>tilinę coniuratione quam uerissime potero pauc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                              image  \\\n",
       "0  010005.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "1  010002.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "2  010007.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "3  010008.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "4  010017.bin.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "\n",
       "                                       transcription  \n",
       "0        initio sicuti pleriq; studio ad empabacan\\n  \n",
       "1       gla memores que s quis: faciliafacto putat\\n  \n",
       "2  pro abstinentia ꝓuirtute audacia. largitio. au...  \n",
       "3  bant. Quę tametsianimus aspꝑnabatur. insolens ...  \n",
       "4  tilinę coniuratione quam uerissime potero pauc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d708f",
   "metadata": {},
   "source": [
    "For now it appear that the images only have \"255\" as values, i.e. white. That's why im checking for other values. But the edges of the images are all white, therefore this is the exspected behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecfc47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with values other than 255: 429\n"
     ]
    }
   ],
   "source": [
    "non_255_values = df['image'].apply(lambda img: np.any(img != 255))\n",
    "print(f\"Rows with values other than 255: {non_255_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cb64f",
   "metadata": {},
   "source": [
    "By calculating the maximum and avg. number of the height we can use that value later in our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dcdfc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum height of an image: 263\n",
      "Average height of an image: 140.2004662004662\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum and average length of the lists in the 'image' column\n",
    "image_lengths = df['image'].apply(lambda img: img.shape[0])  # Get the height of each image array\n",
    "max_length = image_lengths.max()\n",
    "avg_length = image_lengths.mean()\n",
    "\n",
    "print(f\"Maximum height of an image: {max_length}\")\n",
    "print(f\"Average height of an image: {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319becdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (images) and labels (transcriptions)\n",
    "X = df['image'].values  # Images as NumPy arrays\n",
    "y = df['transcription'].values # Transcriptions as strings\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b79a41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in DataFrame: 429\n",
      "Rows with missing images: 0\n",
      "Rows with missing transcriptions: 0\n",
      "Number of samples in X: 429\n",
      "Number of samples in y: 429\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "# Check for missing data in the DataFrame\n",
    "print(f\"Total rows in DataFrame: {len(df)}\")\n",
    "print(f\"Rows with missing images: {df['image'].isnull().sum()}\")\n",
    "print(f\"Rows with missing transcriptions: {df['transcription'].isnull().sum()}\")\n",
    "\n",
    "# Drop rows with missing data\n",
    "df = df.dropna(subset=['image', 'transcription'])\n",
    "\n",
    "# Ensure all arrays have the same number of samples\n",
    "X = df['image'].values  # Images as NumPy arrays\n",
    "y = df['transcription'].values  # Transcriptions as strings\n",
    "\n",
    "print(f\"Number of samples in X: {len(X)}\")\n",
    "print(f\"Number of samples in y: {len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fe5092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 17:21:28.702036: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,610</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │           \u001b[38;5;34m100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │           \u001b[38;5;34m910\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2560\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m25,610\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,730</span> (104.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,730\u001b[0m (104.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,730</span> (104.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,730\u001b[0m (104.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(32, 32, 1)),\n",
    "        layers.Conv2D(10, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        layers.Conv2D(10, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPool2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660556de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (343, 32, 32, 1)\n",
      "X_test shape: (86, 32, 32, 1)\n",
      "y_train shape: (16187, 42846)\n",
      "y_test shape: (4069, 42842)\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "#! This returns each charecter in the transcription as a separate label\n",
    "# Convert labels to categorical format\n",
    "y_train_categorical = keras.utils.to_categorical(\n",
    "    [ord(char) for char in \"\".join(y_train)]\n",
    ")\n",
    "y_test_categorical = keras.utils.to_categorical(\n",
    "    [ord(char) for char in \"\".join(y_test)]\n",
    ")\n",
    "\n",
    "# Reshape the images to match the input shape of the model\n",
    "X_train_reshaped = np.array(\n",
    "    [cv2.resize(img, (32, 32)).reshape(32, 32, 1) for img in X_train]\n",
    ")\n",
    "X_test_reshaped = np.array(\n",
    "    [cv2.resize(img, (32, 32)).reshape(32, 32, 1) for img in X_test]\n",
    ")\n",
    "\n",
    "\n",
    "#DEBUG\n",
    "print(f\"X_train shape: {X_train_reshaped.shape}\")\n",
    "print(f\"X_test shape: {X_test_reshaped.shape}\")\n",
    "print(f\"y_train shape: {y_train_categorical.shape}\")\n",
    "print(f\"y_test shape: {y_test_categorical.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad05303",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 343\n'y' sizes: 16187\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_categorical\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/1_Universitaet/10_SoSe_25/SPECIALIZATION-PROEJECT/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/1_Universitaet/10_SoSe_25/SPECIALIZATION-PROEJECT/.venv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/data_adapter_utils.py:115\u001b[39m, in \u001b[36mcheck_data_cardinality\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    111\u001b[39m     sizes = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    112\u001b[39m         \u001b[38;5;28mstr\u001b[39m(i.shape[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree.flatten(single_data)\n\u001b[32m    113\u001b[39m     )\n\u001b[32m    114\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 343\n'y' sizes: 16187\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(\n",
    "    X_train_reshaped,\n",
    "    y_train_categorical,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test_reshaped, y_test_categorical),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
